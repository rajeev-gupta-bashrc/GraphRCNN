rajeev-gupta@rajeevgupta-Nitro-AN515-58:~$ conda activate graphrcnn
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~$ python tools/create_data.py waymo_data_prep --root_path /data/waymo --processed_data_tag waymo_processed_data_cp --split val --nsweeps 1
python: can't open file 'tools/create_data.py': [Errno 2] No such file or directory
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~$ python tools/create_data(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~$ python tools/create_data.py waymo_data_prep --root_path /media/rajeev-gupta/Ventoy/waymo_data/data/waymo --processed_data_tag waymo_processed_data_cp --split val --nsweeps 1 --procespython: can't open file 'tools/create_data.py': [Errno 2] No such file or directory
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~$ cd sensyn_ws/src/GraphRCNN/
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ python tools/create_data.py waymo_data_prep --root_path /media/rajeev-gupta/Ventoy/waymo_data/data/waymo --processed_data_tag waymo_processed_data_cp --split val --nsweeps 1
val  split  exist frame num: 0
0it [00:00, ?it/s]
sample: 0
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node= ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 564, in determine_local_world_size
    return int(nproc_per_node)
ValueError: invalid literal for int() with base 10: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 709, in run
    config, cmd, cmd_args = config_from_args(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 617, in config_from_args
    nproc_per_node = determine_local_world_size(args.nproc_per_node)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 582, in determine_local_world_size
    raise ValueError(f"Unsupported nproc_per_node value: {nproc_per_node}")
ValueError: Unsupported nproc_per_node value: 
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node= ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 564, in determine_local_world_size
    return int(nproc_per_node)
ValueError: invalid literal for int() with base 10: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 709, in run
    config, cmd, cmd_args = config_from_args(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 617, in config_from_args
    nproc_per_node = determine_local_world_size(args.nproc_per_node)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 582, in determine_local_world_size
    raise ValueError(f"Unsupported nproc_per_node value: {nproc_per_node}")
ValueError: Unsupported nproc_per_node value: 
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 17:21:41,986 - INFO - Distributed testing: False
2024-06-04 17:21:41,986 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 17:21:42,016 - INFO - Finish RPN Initialization
2024-06-04 17:21:42,016 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 17:21:42,020 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 113, in main
    shuffle=False,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 268, in __init__
    sampler = RandomSampler(dataset, generator=generator)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 103, in __init__
    "value, but got num_samples={}".format(self.num_samples))
ValueError: num_samples should be a positive integer value, but got num_samples=0
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node= ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 564, in determine_local_world_size
    return int(nproc_per_node)
ValueError: invalid literal for int() with base 10: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 709, in run
    config, cmd, cmd_args = config_from_args(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 617, in config_from_args
    nproc_per_node = determine_local_world_size(args.nproc_per_node)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 582, in determine_local_world_size
    raise ValueError(f"Unsupported nproc_per_node value: {nproc_per_node}")
ValueError: Unsupported nproc_per_node value: 
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node= ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 564, in determine_local_world_size
    return int(nproc_per_node)
ValueError: invalid literal for int() with base 10: ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 709, in run
    config, cmd, cmd_args = config_from_args(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 617, in config_from_args
    nproc_per_node = determine_local_world_size(args.nproc_per_node)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 582, in determine_local_world_size
    raise ValueError(f"Unsupported nproc_per_node value: {nproc_per_node}")
ValueError: Unsupported nproc_per_node value: 
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 17:31:47,521 - INFO - Distributed testing: False
2024-06-04 17:31:47,521 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 17:31:47,552 - INFO - Finish RPN Initialization
2024-06-04 17:31:47,552 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 17:31:47,556 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 113, in main
    shuffle=False,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 268, in __init__
    sampler = RandomSampler(dataset, generator=generator)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 103, in __init__
    "value, but got num_samples={}".format(self.num_samples))
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 17:39:46,093 - INFO - Distributed training: True
2024-06-04 17:39:46,094 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 17:39:46,145 - INFO - Finish RPN Initialization
2024-06-04 17:39:46,145 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 17:39:46,148 - INFO - Finish CenterHead Initialization
Using 1 sweeps
Using 396 Frames
2024-06-04 17:39:46,863 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 17:39:46,863 - INFO - [-1]
2024-06-04 17:39:46,995 - INFO - load 2212 VEHICLE database infos
2024-06-04 17:39:46,995 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 17:39:46,995 - INFO - load 488 CYCLIST database infos
2024-06-04 17:39:47,001 - INFO - After filter database:
2024-06-04 17:39:47,001 - INFO - load 2054 VEHICLE database infos
2024-06-04 17:39:47,001 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 17:39:47,001 - INFO - load 457 CYCLIST database infos
2024-06-04 17:39:48,987 - INFO - model structure: DistributedDataParallel(
  (module): VoxelNet(
    (reader): DynamicVoxelEncoder()
    (backbone): SpMiddleResNetFHD(
      (conv_input): SparseSequential(
        (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv2): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv3): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv4): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (extra_conv): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (neck): RPN(
      (blocks): ModuleList(
        (0): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
        (1): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
          (2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
      )
      (deblocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
  )
)
2024-06-04 17:39:48,987 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_3x
2024-06-04 17:39:48,987 - INFO - workflow: [('train', 1)], max: 36 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[Exception|implicit_gemm_pair]indices=torch.Size([1589804, 4]),bs=16,ss=[21, 752, 752],algo=ConvAlgo.MaskImplicitGemm,ksize=[3, 3, 3],stride=[2, 2, 2],padding=[1, 1, 1],dilation=[1, 1, 1],subm=False,transpose=False
SPCONV_DEBUG_SAVE_PATH not found, you can specify SPCONV_DEBUG_SAVE_PATH as debug data save path to save debug data which can be attached in a issue.
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 56, in forward
    x, _ = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 47, in extract_feat
    input_features, data["coors"], data["batch_size"], data["input_shape"]
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/backbones/scn.py", line 174, in forward
    x_conv3 = self.conv3(x_conv2)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/modules.py", line 137, in forward
    input = module(input)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 404, in forward
    raise e
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 395, in forward
    timer=input._timer)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/ops.py", line 406, in get_indice_pairs_implicit_gemm
    uniq_res = indice_pairs_uniq.unique()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_tensor.py", line 530, in unique
    return torch.unique(self, sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_jit_internal.py", line 422, in fn
    return if_false(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_jit_internal.py", line 422, in fn
    return if_false(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/functional.py", line 821, in _return_output
    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/functional.py", line 739, in _unique_impl
    return_counts=return_counts,
RuntimeError: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 5.67 GiB total capacity; 4.39 GiB already allocated; 68.69 MiB free; 4.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10102) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_17:40:13
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10102)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 17:40:15,371 - INFO - Distributed training: True
2024-06-04 17:40:15,371 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 17:40:15,399 - INFO - Finish RPN Initialization
2024-06-04 17:40:15,399 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 17:40:15,402 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Using 1 sweeps
Using 396 Frames
2024-06-04 17:40:15,410 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 17:40:15,410 - INFO - [-1]
2024-06-04 17:40:15,418 - INFO - load 2212 VEHICLE database infos
2024-06-04 17:40:15,418 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 17:40:15,418 - INFO - load 488 CYCLIST database infos
2024-06-04 17:40:15,421 - INFO - After filter database:
2024-06-04 17:40:15,421 - INFO - load 2054 VEHICLE database infos
2024-06-04 17:40:15,421 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 17:40:15,421 - INFO - load 457 CYCLIST database infos
2024-06-04 17:40:16,938 - INFO - model structure: DistributedDataParallel(
  (module): TwoStageDetector(
    (single_det): VoxelNet(
      (reader): DynamicVoxelEncoder()
      (backbone): SpMiddleResNetFHD(
        (conv_input): SparseSequential(
          (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=16, eps=0.001)
          (2): ReLU(inplace=True)
        )
        (conv1): SparseSequential(
          (0): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
          (1): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
        )
        (conv2): SparseSequential(
          (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=32, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
        )
        (conv3): SparseSequential(
          (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=64, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
        )
        (conv4): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
        )
        (extra_conv): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU()
        )
      )
      (neck): RPN(
        (blocks): ModuleList(
          (0): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
            (2): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (3): ReLU()
            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (6): ReLU()
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (9): ReLU()
            (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (12): ReLU()
            (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (15): ReLU()
            (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (18): ReLU()
          )
          (1): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
            (2): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (3): ReLU()
            (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (6): ReLU()
            (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (9): ReLU()
            (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (12): ReLU()
            (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (15): ReLU()
            (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (18): ReLU()
          )
        )
        (deblocks): ModuleList(
          (0): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
        )
      )
      (bbox_head): CenterHead(
        (crit): FastFocalLoss()
        (crit_reg): RegLoss()
        (shared_conv): Sequential(
          (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          (2): ReLU(inplace=True)
        )
        (tasks): ModuleList(
          (0): SepHead(
            (reg): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (height): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (dim): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (rot): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (hm): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (roi_head): GraphRCNNHead(
      (proposal_target_layer): ProposalTargetLayer()
      (roilocal_dfvs_pool3d_layer): RoILocalDFVSPool3d()
      (attn_gnn_layer): AttnGNNLayer(
        (edge_layes): ModuleList(
          (0): Sequential(
            (0): Conv2d(22, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (calib): Sequential(
          (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (expansion): Sequential(
          (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (reduction): Sequential(
          (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (shortcut): ShortcutLayer(
          (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (norm1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (shared_fc_layer): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (cls_layers): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
      (reg_layers): Conv1d(256, 7, kernel_size=(1,), stride=(1,))
    )
  )
)
2024-06-04 17:40:16,939 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
2024-06-04 17:40:16,939 - INFO - workflow: [('train', 1)], max: 6 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/two_stage.py", line 110, in forward
    return_loss, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 65, in forward_two_stage
    x, voxel_feature = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 51, in extract_feat
    x = self.neck(x)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/necks/rpn.py", line 155, in forward
    ups.append(self.deblocks[i - self._upsample_start_idx](x))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/utils/misc.py", line 93, in forward
    input = module(input)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/utils/finetune_utils.py", line 54, in forward
    eps=self.eps,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/functional.py", line 2283, in batch_norm
    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 554.00 MiB (GPU 0; 5.67 GiB total capacity; 3.49 GiB already allocated; 164.69 MiB free; 4.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10204) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_17:40:44
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10204)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 17:40:45,458 - INFO - Distributed testing: False
2024-06-04 17:40:45,458 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 17:40:45,489 - INFO - Finish RPN Initialization
2024-06-04 17:40:45,489 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 17:40:45,493 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 113, in main
    shuffle=False,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 268, in __init__
    sampler = RandomSampler(dataset, generator=generator)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 103, in __init__
    "value, but got num_samples={}".format(self.num_samples))
ValueError: num_samples should be a positive integer value, but got num_samples=0
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 18:40:45,269 - INFO - Distributed training: True
2024-06-04 18:40:45,269 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 18:40:45,314 - INFO - Finish RPN Initialization
2024-06-04 18:40:45,314 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 18:40:45,317 - INFO - Finish CenterHead Initialization
Using 1 sweeps
Using 396 Frames
2024-06-04 18:40:45,320 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 18:40:45,320 - INFO - [-1]
2024-06-04 18:40:45,328 - INFO - load 2212 VEHICLE database infos
2024-06-04 18:40:45,328 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 18:40:45,328 - INFO - load 488 CYCLIST database infos
2024-06-04 18:40:45,331 - INFO - After filter database:
2024-06-04 18:40:45,331 - INFO - load 2054 VEHICLE database infos
2024-06-04 18:40:45,331 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 18:40:45,331 - INFO - load 457 CYCLIST database infos
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 256, in train_detector
    for ds in dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 256, in <listcomp>
    for ds in dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 245, in __init__
    raise ValueError('sampler option is mutually exclusive with '
ValueError: sampler option is mutually exclusive with shuffle
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10778) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_18:40:47
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10778)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 18:40:49,265 - INFO - Distributed training: True
2024-06-04 18:40:49,265 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 18:40:49,310 - INFO - Finish RPN Initialization
2024-06-04 18:40:49,311 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 18:40:49,314 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Using 1 sweeps
Using 396 Frames
2024-06-04 18:40:49,321 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 18:40:49,321 - INFO - [-1]
2024-06-04 18:40:49,330 - INFO - load 2212 VEHICLE database infos
2024-06-04 18:40:49,330 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 18:40:49,330 - INFO - load 488 CYCLIST database infos
2024-06-04 18:40:49,333 - INFO - After filter database:
2024-06-04 18:40:49,333 - INFO - load 2054 VEHICLE database infos
2024-06-04 18:40:49,333 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 18:40:49,333 - INFO - load 457 CYCLIST database infos
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 256, in train_detector
    for ds in dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 256, in <listcomp>
    for ds in dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 245, in __init__
    raise ValueError('sampler option is mutually exclusive with '
ValueError: sampler option is mutually exclusive with shuffle
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10795) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_18:40:53
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10795)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 18:40:54,402 - INFO - Distributed testing: False
2024-06-04 18:40:54,402 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 18:40:54,433 - INFO - Finish RPN Initialization
2024-06-04 18:40:54,433 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 18:40:54,437 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 116, in main
    checkpoint = load_checkpoint(model, args.checkpoint, map_location="cpu")
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/checkpoint.py", line 200, in load_checkpoint
    raise IOError("{} is not a checkpoint file".format(filename))
OSError: work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth is not a checkpoint file
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:02:41,219 - INFO - Distributed training: True
2024-06-04 19:02:41,219 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:02:41,269 - INFO - Finish RPN Initialization
2024-06-04 19:02:41,269 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:02:41,272 - INFO - Finish CenterHead Initialization
Using 1 sweeps
Using 396 Frames
2024-06-04 19:02:41,275 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:02:41,275 - INFO - [-1]
2024-06-04 19:02:41,284 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:02:41,284 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:02:41,284 - INFO - load 488 CYCLIST database infos
2024-06-04 19:02:41,287 - INFO - After filter database:
2024-06-04 19:02:41,287 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:02:41,287 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:02:41,287 - INFO - load 457 CYCLIST database infos
2024-06-04 19:02:42,784 - INFO - model structure: DistributedDataParallel(
  (module): VoxelNet(
    (reader): DynamicVoxelEncoder()
    (backbone): SpMiddleResNetFHD(
      (conv_input): SparseSequential(
        (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv2): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv3): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv4): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (extra_conv): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (neck): RPN(
      (blocks): ModuleList(
        (0): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
        (1): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
          (2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
      )
      (deblocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
  )
)
2024-06-04 19:02:42,785 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_3x
2024-06-04 19:02:42,785 - INFO - workflow: [('train', 1)], max: 36 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[Exception|implicit_gemm]feat=torch.Size([1540641, 32]),w=torch.Size([64, 3, 3, 3, 32]),pair=torch.Size([27, 840870]),act=840870,issubm=False,istrain=True
SPCONV_DEBUG_SAVE_PATH not found, you can specify SPCONV_DEBUG_SAVE_PATH as debug data save path to save debug data which can be attached in a issue.
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 56, in forward
    x, _ = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 47, in extract_feat
    input_features, data["coors"], data["batch_size"], data["input_shape"]
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/backbones/scn.py", line 174, in forward
    x_conv3 = self.conv3(x_conv2)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/modules.py", line 137, in forward
    input = module(input)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 446, in forward
    input._timer, self.fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 94, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/functional.py", line 200, in forward
    raise e 
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/functional.py", line 191, in forward
    fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/ops.py", line 1118, in implicit_gemm
    fp32_accum=fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/algo.py", line 618, in tune_and_cache
    inp = inp.clone()
RuntimeError: TensorStorage /io/include/tensorview/tensor.h 168
cuda failed with error 2 out of memory. use CUDA_LAUNCH_BLOCKING=1 to get correct traceback.

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10939) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:03:08
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10939)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:03:10,358 - INFO - Distributed training: True
2024-06-04 19:03:10,358 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:03:10,407 - INFO - Finish RPN Initialization
2024-06-04 19:03:10,407 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:03:10,410 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Using 1 sweeps
Using 396 Frames
2024-06-04 19:03:10,418 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:03:10,418 - INFO - [-1]
2024-06-04 19:03:10,426 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:03:10,426 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:03:10,426 - INFO - load 488 CYCLIST database infos
2024-06-04 19:03:10,429 - INFO - After filter database:
2024-06-04 19:03:10,430 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:03:10,430 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:03:10,430 - INFO - load 457 CYCLIST database infos
2024-06-04 19:03:11,998 - INFO - model structure: DistributedDataParallel(
  (module): TwoStageDetector(
    (single_det): VoxelNet(
      (reader): DynamicVoxelEncoder()
      (backbone): SpMiddleResNetFHD(
        (conv_input): SparseSequential(
          (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=16, eps=0.001)
          (2): ReLU(inplace=True)
        )
        (conv1): SparseSequential(
          (0): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
          (1): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
        )
        (conv2): SparseSequential(
          (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=32, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
        )
        (conv3): SparseSequential(
          (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=64, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
        )
        (conv4): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
        )
        (extra_conv): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU()
        )
      )
      (neck): RPN(
        (blocks): ModuleList(
          (0): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
            (2): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (3): ReLU()
            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (6): ReLU()
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (9): ReLU()
            (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (12): ReLU()
            (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (15): ReLU()
            (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (18): ReLU()
          )
          (1): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
            (2): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (3): ReLU()
            (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (6): ReLU()
            (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (9): ReLU()
            (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (12): ReLU()
            (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (15): ReLU()
            (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (18): ReLU()
          )
        )
        (deblocks): ModuleList(
          (0): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
        )
      )
      (bbox_head): CenterHead(
        (crit): FastFocalLoss()
        (crit_reg): RegLoss()
        (shared_conv): Sequential(
          (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          (2): ReLU(inplace=True)
        )
        (tasks): ModuleList(
          (0): SepHead(
            (reg): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (height): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (dim): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (rot): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (hm): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (roi_head): GraphRCNNHead(
      (proposal_target_layer): ProposalTargetLayer()
      (roilocal_dfvs_pool3d_layer): RoILocalDFVSPool3d()
      (attn_gnn_layer): AttnGNNLayer(
        (edge_layes): ModuleList(
          (0): Sequential(
            (0): Conv2d(22, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (calib): Sequential(
          (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (expansion): Sequential(
          (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (reduction): Sequential(
          (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (shortcut): ShortcutLayer(
          (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (norm1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (shared_fc_layer): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (cls_layers): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
      (reg_layers): Conv1d(256, 7, kernel_size=(1,), stride=(1,))
    )
  )
)
2024-06-04 19:03:11,999 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
2024-06-04 19:03:11,999 - INFO - workflow: [('train', 1)], max: 6 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/two_stage.py", line 110, in forward
    return_loss, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 65, in forward_two_stage
    x, voxel_feature = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 51, in extract_feat
    x = self.neck(x)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/necks/rpn.py", line 157, in forward
    x = torch.cat(ups, dim=1)
RuntimeError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 5.67 GiB total capacity; 3.52 GiB already allocated; 604.69 MiB free; 3.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11038) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:03:34
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 11038)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 19:03:35,474 - INFO - Distributed testing: False
2024-06-04 19:03:35,474 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:03:35,524 - INFO - Finish RPN Initialization
2024-06-04 19:03:35,524 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:03:35,528 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 113, in main
    shuffle=True,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/build_loader.py", line 54, in build_dataloader
    pin_memory=False,
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 268, in __init__
    sampler = RandomSampler(dataset, generator=generator)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 103, in __init__
    "value, but got num_samples={}".format(self.num_samples))
ValueError: num_samples should be a positive integer value, but got num_samples=0
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:06:51,237 - INFO - Distributed training: True
2024-06-04 19:06:51,237 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:06:51,290 - INFO - Finish RPN Initialization
2024-06-04 19:06:51,290 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:06:51,294 - INFO - Finish CenterHead Initialization
Using 1 sweeps
Using 396 Frames
2024-06-04 19:06:51,296 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:06:51,296 - INFO - [-1]
2024-06-04 19:06:51,306 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:06:51,306 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:06:51,306 - INFO - load 488 CYCLIST database infos
2024-06-04 19:06:51,309 - INFO - After filter database:
2024-06-04 19:06:51,309 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:06:51,309 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:06:51,309 - INFO - load 457 CYCLIST database infos
2024-06-04 19:06:52,830 - INFO - model structure: DistributedDataParallel(
  (module): VoxelNet(
    (reader): DynamicVoxelEncoder()
    (backbone): SpMiddleResNetFHD(
      (conv_input): SparseSequential(
        (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv2): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv3): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv4): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (extra_conv): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (neck): RPN(
      (blocks): ModuleList(
        (0): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
        (1): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
          (2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
      )
      (deblocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
  )
)
2024-06-04 19:06:52,830 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_3x
2024-06-04 19:06:52,831 - INFO - workflow: [('train', 1)], max: 36 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[Exception|implicit_gemm_pair]indices=torch.Size([1576387, 4]),bs=16,ss=[21, 752, 752],algo=ConvAlgo.MaskImplicitGemm,ksize=[3, 3, 3],stride=[2, 2, 2],padding=[1, 1, 1],dilation=[1, 1, 1],subm=False,transpose=False
SPCONV_DEBUG_SAVE_PATH not found, you can specify SPCONV_DEBUG_SAVE_PATH as debug data save path to save debug data which can be attached in a issue.
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 56, in forward
    x, _ = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 47, in extract_feat
    input_features, data["coors"], data["batch_size"], data["input_shape"]
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/backbones/scn.py", line 174, in forward
    x_conv3 = self.conv3(x_conv2)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/modules.py", line 137, in forward
    input = module(input)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 404, in forward
    raise e
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 395, in forward
    timer=input._timer)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/ops.py", line 406, in get_indice_pairs_implicit_gemm
    uniq_res = indice_pairs_uniq.unique()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_tensor.py", line 530, in unique
    return torch.unique(self, sorted=sorted, return_inverse=return_inverse, return_counts=return_counts, dim=dim)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_jit_internal.py", line 422, in fn
    return if_false(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/_jit_internal.py", line 422, in fn
    return if_false(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/functional.py", line 821, in _return_output
    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/functional.py", line 739, in _unique_impl
    return_counts=return_counts,
RuntimeError: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 5.67 GiB total capacity; 4.35 GiB already allocated; 94.69 MiB free; 4.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11169) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:07:13
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 11169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:07:15,467 - INFO - Distributed training: True
2024-06-04 19:07:15,467 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:07:15,497 - INFO - Finish RPN Initialization
2024-06-04 19:07:15,498 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:07:15,501 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Using 1 sweeps
Using 396 Frames
2024-06-04 19:07:15,509 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:07:15,509 - INFO - [-1]
2024-06-04 19:07:15,518 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:07:15,518 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:07:15,518 - INFO - load 488 CYCLIST database infos
2024-06-04 19:07:15,521 - INFO - After filter database:
2024-06-04 19:07:15,521 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:07:15,521 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:07:15,521 - INFO - load 457 CYCLIST database infos
2024-06-04 19:07:17,074 - INFO - model structure: DistributedDataParallel(
  (module): TwoStageDetector(
    (single_det): VoxelNet(
      (reader): DynamicVoxelEncoder()
      (backbone): SpMiddleResNetFHD(
        (conv_input): SparseSequential(
          (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=16, eps=0.001)
          (2): ReLU(inplace=True)
        )
        (conv1): SparseSequential(
          (0): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
          (1): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
        )
        (conv2): SparseSequential(
          (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=32, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
        )
        (conv3): SparseSequential(
          (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=64, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
        )
        (conv4): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
        )
        (extra_conv): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU()
        )
      )
      (neck): RPN(
        (blocks): ModuleList(
          (0): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
            (2): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (3): ReLU()
            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (6): ReLU()
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (9): ReLU()
            (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (12): ReLU()
            (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (15): ReLU()
            (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (18): ReLU()
          )
          (1): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
            (2): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (3): ReLU()
            (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (6): ReLU()
            (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (9): ReLU()
            (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (12): ReLU()
            (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (15): ReLU()
            (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (18): ReLU()
          )
        )
        (deblocks): ModuleList(
          (0): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
        )
      )
      (bbox_head): CenterHead(
        (crit): FastFocalLoss()
        (crit_reg): RegLoss()
        (shared_conv): Sequential(
          (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          (2): ReLU(inplace=True)
        )
        (tasks): ModuleList(
          (0): SepHead(
            (reg): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (height): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (dim): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (rot): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (hm): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (roi_head): GraphRCNNHead(
      (proposal_target_layer): ProposalTargetLayer()
      (roilocal_dfvs_pool3d_layer): RoILocalDFVSPool3d()
      (attn_gnn_layer): AttnGNNLayer(
        (edge_layes): ModuleList(
          (0): Sequential(
            (0): Conv2d(22, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (calib): Sequential(
          (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (expansion): Sequential(
          (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (reduction): Sequential(
          (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (shortcut): ShortcutLayer(
          (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (norm1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (shared_fc_layer): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (cls_layers): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
      (reg_layers): Conv1d(256, 7, kernel_size=(1,), stride=(1,))
    )
  )
)
2024-06-04 19:07:17,075 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
2024-06-04 19:07:17,075 - INFO - workflow: [('train', 1)], max: 6 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/two_stage.py", line 110, in forward
    return_loss, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 65, in forward_two_stage
    x, voxel_feature = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 51, in extract_feat
    x = self.neck(x)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/necks/rpn.py", line 157, in forward
    x = torch.cat(ups, dim=1)
RuntimeError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 5.67 GiB total capacity; 3.50 GiB already allocated; 616.69 MiB free; 3.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11273) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:07:39
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 11273)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 19:07:40,516 - INFO - Distributed testing: False
2024-06-04 19:07:40,516 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:07:40,548 - INFO - Finish RPN Initialization
2024-06-04 19:07:40,548 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:07:40,552 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
Traceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 116, in main
    checkpoint = load_checkpoint(model, args.checkpoint, map_location="cpu")
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/checkpoint.py", line 200, in load_checkpoint
    raise IOError("{} is not a checkpoint file".format(filename))
OSError: work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth is not a checkpoint file
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:35:00,434 - INFO - Distributed training: True
2024-06-04 19:35:00,434 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:35:00,483 - INFO - Finish RPN Initialization
2024-06-04 19:35:00,483 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:35:00,486 - INFO - Finish CenterHead Initialization
Using 1 sweeps
Using 396 Frames
2024-06-04 19:35:00,489 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:35:00,489 - INFO - [-1]
2024-06-04 19:35:00,497 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:35:00,497 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:35:00,497 - INFO - load 488 CYCLIST database infos
2024-06-04 19:35:00,500 - INFO - After filter database:
2024-06-04 19:35:00,500 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:35:00,500 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:35:00,500 - INFO - load 457 CYCLIST database infos
2024-06-04 19:35:02,005 - INFO - model structure: DistributedDataParallel(
  (module): VoxelNet(
    (reader): DynamicVoxelEncoder()
    (backbone): SpMiddleResNetFHD(
      (conv_input): SparseSequential(
        (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv2): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv3): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (conv4): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
        (4): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU()
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
          (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (extra_conv): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (neck): RPN(
      (blocks): ModuleList(
        (0): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
          (2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
        (1): Sequential(
          (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
          (2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (5): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (9): ReLU()
          (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (11): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (12): ReLU()
          (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (14): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (15): ReLU()
          (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (17): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (18): ReLU()
        )
      )
      (deblocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU()
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
  )
)
2024-06-04 19:35:02,005 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_3x
2024-06-04 19:35:02,005 - INFO - workflow: [('train', 1)], max: 36 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[Exception|implicit_gemm]feat=torch.Size([1544829, 32]),w=torch.Size([64, 3, 3, 3, 32]),pair=torch.Size([27, 847328]),act=847328,issubm=False,istrain=True
SPCONV_DEBUG_SAVE_PATH not found, you can specify SPCONV_DEBUG_SAVE_PATH as debug data save path to save debug data which can be attached in a issue.
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 56, in forward
    x, _ = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 47, in extract_feat
    input_features, data["coors"], data["batch_size"], data["input_shape"]
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/backbones/scn.py", line 174, in forward
    x_conv3 = self.conv3(x_conv2)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/modules.py", line 137, in forward
    input = module(input)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/conv.py", line 446, in forward
    input._timer, self.fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py", line 94, in decorate_fwd
    return fwd(*args, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/functional.py", line 200, in forward
    raise e 
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/functional.py", line 191, in forward
    fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/pytorch/ops.py", line 1118, in implicit_gemm
    fp32_accum=fp32_accum)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/spconv/algo.py", line 618, in tune_and_cache
    inp = inp.clone()
RuntimeError: TensorStorage /io/include/tensorview/tensor.h 168
cuda failed with error 2 out of memory. use CUDA_LAUNCH_BLOCKING=1 to get correct traceback.

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 12095) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:35:22
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 12095)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
2024-06-04 19:35:24,591 - INFO - Distributed training: True
2024-06-04 19:35:24,591 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:35:24,621 - INFO - Finish RPN Initialization
2024-06-04 19:35:24,621 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:35:24,624 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Using 1 sweeps
Using 396 Frames
2024-06-04 19:35:24,632 - INFO - {'VEHICLE': 5, 'PEDESTRIAN': 5, 'CYCLIST': 5}
2024-06-04 19:35:24,632 - INFO - [-1]
2024-06-04 19:35:24,640 - INFO - load 2212 VEHICLE database infos
2024-06-04 19:35:24,640 - INFO - load 1396 PEDESTRIAN database infos
2024-06-04 19:35:24,640 - INFO - load 488 CYCLIST database infos
2024-06-04 19:35:24,643 - INFO - After filter database:
2024-06-04 19:35:24,643 - INFO - load 2054 VEHICLE database infos
2024-06-04 19:35:24,643 - INFO - load 1371 PEDESTRIAN database infos
2024-06-04 19:35:24,643 - INFO - load 457 CYCLIST database infos
2024-06-04 19:35:26,178 - INFO - model structure: DistributedDataParallel(
  (module): TwoStageDetector(
    (single_det): VoxelNet(
      (reader): DynamicVoxelEncoder()
      (backbone): SpMiddleResNetFHD(
        (conv_input): SparseSequential(
          (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=16, eps=0.001)
          (2): ReLU(inplace=True)
        )
        (conv1): SparseSequential(
          (0): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
          (1): SparseBasicBlock(
            (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=16, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=16, eps=0.001)
          )
        )
        (conv2): SparseSequential(
          (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=32, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=32, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=32, eps=0.001)
          )
        )
        (conv3): SparseSequential(
          (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=64, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=64, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=64, eps=0.001)
          )
        )
        (conv4): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU(inplace=True)
          (3): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
          (4): SparseBasicBlock(
            (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn1): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (relu): ReLU()
            (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (bn2): FrozenBatchNorm2d(num_features=128, eps=0.001)
          )
        )
        (extra_conv): SparseSequential(
          (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): FrozenBatchNorm2d(num_features=128, eps=0.001)
          (2): ReLU()
        )
      )
      (neck): RPN(
        (blocks): ModuleList(
          (0): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)
            (2): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (3): ReLU()
            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (6): ReLU()
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (9): ReLU()
            (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (12): ReLU()
            (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (15): ReLU()
            (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=128, eps=0.001)
            (18): ReLU()
          )
          (1): Sequential(
            (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
            (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)
            (2): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (3): ReLU()
            (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (5): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (6): ReLU()
            (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (8): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (9): ReLU()
            (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (11): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (12): ReLU()
            (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (14): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (15): ReLU()
            (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (17): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (18): ReLU()
          )
        )
        (deblocks): ModuleList(
          (0): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(num_features=256, eps=0.001)
            (2): ReLU()
          )
        )
      )
      (bbox_head): CenterHead(
        (crit): FastFocalLoss()
        (crit_reg): RegLoss()
        (shared_conv): Sequential(
          (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          (2): ReLU(inplace=True)
        )
        (tasks): ModuleList(
          (0): SepHead(
            (reg): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (height): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (dim): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (rot): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (hm): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              (2): ReLU()
              (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
        )
      )
    )
    (bbox_head): CenterHead(
      (crit): FastFocalLoss()
      (crit_reg): RegLoss()
      (shared_conv): Sequential(
        (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        (2): ReLU(inplace=True)
      )
      (tasks): ModuleList(
        (0): SepHead(
          (reg): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (hm): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            (2): ReLU()
            (3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (roi_head): GraphRCNNHead(
      (proposal_target_layer): ProposalTargetLayer()
      (roilocal_dfvs_pool3d_layer): RoILocalDFVSPool3d()
      (attn_gnn_layer): AttnGNNLayer(
        (edge_layes): ModuleList(
          (0): Sequential(
            (0): Conv2d(22, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (calib): Sequential(
          (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (expansion): Sequential(
          (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (reduction): Sequential(
          (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
        )
        (shortcut): ShortcutLayer(
          (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (norm1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (shared_fc_layer): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (cls_layers): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
      (reg_layers): Conv1d(256, 7, kernel_size=(1,), stride=(1,))
    )
  )
)
2024-06-04 19:35:26,179 - INFO - Start running, host: rajeev-gupta@rajeevgupta-Nitro-AN515-58, work_dir: /home/rajeev-gupta/sensyn_ws/src/GraphRCNN/work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
2024-06-04 19:35:26,179 - INFO - workflow: [('train', 1)], max: 6 epochs
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
[WARNING]your gpu arch (8, 9) isn't compiled in prebuilt, may cause invalid device function. available: {(7, 0), (6, 1), (8, 0), (6, 0), (7, 5), (8, 6), (5, 2)}
Traceback (most recent call last):
  File "./tools/train.py", line 158, in <module>
    main()
  File "./tools/train.py", line 153, in main
    logger=logger,
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/apis/train.py", line 320, in train_detector
    trainer.run(data_loaders, cfg.workflow, cfg.total_epochs, local_rank=cfg.local_rank)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 542, in run
    epoch_runner(data_loaders[i], self.epoch, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 409, in train
    self.model, data_batch, train_mode=True, **kwargs
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/trainer.py", line 367, in batch_processor_inline
    losses = model(example, return_loss=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/two_stage.py", line 110, in forward
    return_loss, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 65, in forward_two_stage
    x, voxel_feature = self.extract_feat(example)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/detectors/voxelnet.py", line 51, in extract_feat
    x = self.neck(x)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/models/necks/rpn.py", line 157, in forward
    x = torch.cat(ups, dim=1)
RuntimeError: CUDA out of memory. Tried to allocate 1.08 GiB (GPU 0; 5.67 GiB total capacity; 3.48 GiB already allocated; 644.69 MiB free; 3.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 12195) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_19:35:48
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 12195)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
2024-06-04 19:35:49,639 - INFO - Distributed testing: False
2024-06-04 19:35:49,639 - INFO - torch.backends.cudnn.benchmark: False
2024-06-04 19:35:49,670 - INFO - Finish RPN Initialization
2024-06-04 19:35:49,670 - INFO - num_classes: [3]
Use HM Bias:  -2.19
2024-06-04 19:35:49,674 - INFO - Finish CenterHead Initialization
no pretrained model at work_dirs/waymo_centerpoint_voxelnet_3x/epoch_36.pth
Freeze First Stage Network
Use Val Set
Using 1 sweeps
Using 0 Frames
2024-06-04 19:35:51,194 - INFO - work dir: work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
completed: 0, elapsed: 0sTraceback (most recent call last):
  File "./tools/dist_test.py", line 204, in <module>
    main()
  File "./tools/dist_test.py", line 150, in main
    for i, data_batch in enumerate(data_loader):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 359, in __iter__
    return self._get_iterator()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 944, in __init__
    self._reset(loader, first_iter=True)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 975, in _reset
    self._try_put_index()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1209, in _try_put_index
    index = self._next_index()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 512, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/sampler.py", line 229, in __iter__
    for idx in self.sampler:
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/loader/sampler.py", line 125, in __iter__
    indices = np.concatenate(indices)
  File "<__array_function__ internals>", line 6, in concatenate
ValueError: need at least one array to concatenate
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 12347) is killed by signal: Terminated. 
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ ^C
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ 




























(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ bash dist_trainval.sh 
+ NGPUS=1
+ CFG_NAME=waymo_centerpoint_voxelnet_3x
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/waymo_centerpoint_voxelnet_3x.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 11, in <module>
    import chardet
ModuleNotFoundError: No module named 'chardet'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./tools/train.py", line 14, in <module>
    from det3d.datasets import build_dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/__init__.py", line 1, in <module>
    from .builder import build_dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/builder.py", line 3, in <module>
    from det3d.utils import build_from_cfg
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/utils/__init__.py", line 2, in <module>
    from .registry import Registry, build_from_cfg
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/utils/registry.py", line 3, in <module>
    from det3d import torchie
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/__init__.py", line 2, in <module>
    from .cnn import *
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/__init__.py", line 1, in <module>
    from .alexnet import AlexNet
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/alexnet.py", line 5, in <module>
    from ..trainer import load_checkpoint
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/__init__.py", line 7, in <module>
    from .hooks import (
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/__init__.py", line 5, in <module>
    from .logger import LoggerHook, PaviLoggerHook, TensorboardLoggerHook, TextLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/__init__.py", line 2, in <module>
    from .pavi import PaviLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/pavi.py", line 10, in <module>
    import requests
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/__init__.py", line 45, in <module>
    from .exceptions import RequestsDependencyWarning
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/exceptions.py", line 9, in <module>
    from .compat import JSONDecodeError as CompatJSONDecodeError
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 13, in <module>
    import charset_normalizer as chardet
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/__init__.py", line 23, in <module>
    from charset_normalizer.api import from_fp, from_path, from_bytes, normalize
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/api.py", line 10, in <module>
    from charset_normalizer.md import mess_ratio
  File "charset_normalizer/md.py", line 5, in <module>
ImportError: cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/constant.py)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 44368) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-05_01:15:18
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 44368)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ CFG_NAME=waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze
+ python -m torch.distributed.launch --nproc_per_node=1 ./tools/train.py --launcher pytorch configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py
/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 11, in <module>
    import chardet
ModuleNotFoundError: No module named 'chardet'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./tools/train.py", line 14, in <module>
    from det3d.datasets import build_dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/__init__.py", line 1, in <module>
    from .builder import build_dataset
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/datasets/builder.py", line 3, in <module>
    from det3d.utils import build_from_cfg
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/utils/__init__.py", line 2, in <module>
    from .registry import Registry, build_from_cfg
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/utils/registry.py", line 3, in <module>
    from det3d import torchie
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/__init__.py", line 2, in <module>
    from .cnn import *
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/__init__.py", line 1, in <module>
    from .alexnet import AlexNet
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/alexnet.py", line 5, in <module>
    from ..trainer import load_checkpoint
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/__init__.py", line 7, in <module>
    from .hooks import (
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/__init__.py", line 5, in <module>
    from .logger import LoggerHook, PaviLoggerHook, TensorboardLoggerHook, TextLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/__init__.py", line 2, in <module>
    from .pavi import PaviLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/pavi.py", line 10, in <module>
    import requests
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/__init__.py", line 45, in <module>
    from .exceptions import RequestsDependencyWarning
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/exceptions.py", line 9, in <module>
    from .compat import JSONDecodeError as CompatJSONDecodeError
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 13, in <module>
    import charset_normalizer as chardet
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/__init__.py", line 23, in <module>
    from charset_normalizer.api import from_fp, from_path, from_bytes, normalize
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/api.py", line 10, in <module>
    from charset_normalizer.md import mess_ratio
  File "charset_normalizer/md.py", line 5, in <module>
ImportError: cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/constant.py)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 44376) of binary: /home/rajeev-gupta/miniconda3/envs/graphrcnn/bin/python
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/run.py", line 713, in run
    )(*cmd_args)
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-05_01:15:24
  host      : rajeevgupta-Nitro-AN515-58
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 44376)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
+ python ./tools/dist_test.py configs/waymo/voxelnet/two_stage/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze.py --work_dir work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze --checkpoint work_dirs/waymo_centerpoint_voxelnet_graphrcnn_6epoch_freeze/latest.pth
Traceback (most recent call last):
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 11, in <module>
    import chardet
ModuleNotFoundError: No module named 'chardet'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./tools/dist_test.py", line 9, in <module>
    from det3d import torchie
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/__init__.py", line 2, in <module>
    from .cnn import *
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/__init__.py", line 1, in <module>
    from .alexnet import AlexNet
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/cnn/alexnet.py", line 5, in <module>
    from ..trainer import load_checkpoint
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/__init__.py", line 7, in <module>
    from .hooks import (
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/__init__.py", line 5, in <module>
    from .logger import LoggerHook, PaviLoggerHook, TensorboardLoggerHook, TextLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/__init__.py", line 2, in <module>
    from .pavi import PaviLoggerHook
  File "/home/rajeev-gupta/sensyn_ws/src/GraphRCNN/det3d/torchie/trainer/hooks/logger/pavi.py", line 10, in <module>
    import requests
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/__init__.py", line 45, in <module>
    from .exceptions import RequestsDependencyWarning
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/exceptions.py", line 9, in <module>
    from .compat import JSONDecodeError as CompatJSONDecodeError
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/requests/compat.py", line 13, in <module>
    import charset_normalizer as chardet
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/__init__.py", line 23, in <module>
    from charset_normalizer.api import from_fp, from_path, from_bytes, normalize
  File "/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/api.py", line 10, in <module>
    from charset_normalizer.md import mess_ratio
  File "charset_normalizer/md.py", line 5, in <module>
ImportError: cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (/home/rajeev-gupta/miniconda3/envs/graphrcnn/lib/python3.7/site-packages/charset_normalizer/constant.py)
(graphrcnn) rajeev-gupta@rajeevgupta-Nitro-AN515-58:~/sensyn_ws/src/GraphRCNN$ 


